---
layout:     post
title:      神经网络与深度学习小结（一）
subtitle:   神经网络与深度学习
date:       2018-02-11
author:     sunlianglong
header-img: img/post-bg-universe.jpg
catalog: true
tags:
    - 神经网络
    - 深度学习
    - python
---

　　最近学习了**Andrew Ng**在网易云开放的[深度学习与神经网络](http://mooc.study.163.com/smartSpec/detail/1001319001.htm "深度学习与神经网络")课程，受益匪浅，特来做一个初学者的总结。
## 神经网络

　　其实在学习神经网络之前，掌握基本的机器学习知识很重要，要不学起来还是比较吃力，从监督学习的**梯度下降算法**（对后面的学习应用很广泛），到极大似然估计，从激活函数的种类与选择，到损失函数的确定，以及正则化，dropout等等防止学习过拟合的方法，在神经网络与深度学习的训练预测过程中，都发挥着比较大比较基础的作用。

　　基础神经网络的学习参考网址：
- [神经网络浅讲：从神经元到深度学习](http://www.cnblogs.com/subconscious/p/5058741.html "神经网络浅讲：从神经元到深度学习")
- [如何简单形象又有趣地讲解神经网络是什么？](https://www.zhihu.com/question/22553761 "如何简单形象又有趣地讲解神经网络是什么？")


以**两层神经网络**为例：

　　两层神经网络除了包含一个输入层，一个输出层以外，还增加了一个中间层。此时，中间层和输出层都是计算层。我们扩展上节的单层神经网络，在右边新加一个层次（只含有一个节点）。

　　现在，我们的权值矩阵增加到了两个，我们用上标来区分不同层次之间的变量。
### Forward and backward functions
<center>
![](http://myblog-1253290602.file.myqcloud.com/longlong-blog/1-1024x534.png)
</center>


- 使用**矩阵运算**来表达整个计算公式的话如下：

$$g(W^{(1)} * a^{(1)}) = a^{(2)}$$

$$g(W^{(2)} * a^{(2)}) = z$$

- 由此可见，使用矩阵运算来表达是很简洁的，而且也**不会受到节点数增多的影响**（无论有多少节点参与运算，乘法两端都只有一个变量）。因此神经网络的教程中大量使用矩阵运算来描述。

- 我们对神经网络的结构图的讨论中都没有提到**偏置节点**（bias unit）。事实上，这些节点是默认存在的。它本质上是一个只含有存储功能，且存储值永远为1的单元。在神经网络的每个层次中，除了输出层以外，都会含有这样一个偏置单元。正如线性回归模型与逻辑回归模型中的一样。
<center>
<img src="http://myblog-1253290602.file.myqcloud.com/longlong-blog/network1.jpg" width = "400" height = "400"/>
</center>

- 考虑了偏置以后的一个神经网络的矩阵运算如下：

$$g(W^{(1)} * a^{(1)} + b^{(1)}) = a^{(2)}$$

$$g(W^{(2)} * a^{(2)} + b^{(2)}) = z$$

- 事实上，神经网络的本质就是通过参数与激活函数来拟合特征与目标之间的真实函数关系。初学者可能认为画神经网络的结构图是为了在程序中实现这些圆圈与线，但在一个神经网络的程序中，既没有“线”这个对象，也没有“单元”这个对象。实现一个神经网络最需要的是线性代数库。

　　神经网络每一层都包含多个单元。输入层的每个单元对应于**用于训练的元组的每个特征**，输入层的单元，经过加权后，提供给隐藏层的单元。而隐藏层的输出又可以是另一个隐藏层，层层递进，构成了深度神经网络。当然也可以之有一层印隐藏层。
之所以称该网络为前馈的，是因为权重不回送到输入层或前一层的输出单元。

## 数据变换

　　之所以说两层与单层神经网络不同，两层神经网络可以无限逼近任意连续函数，单层网络只能做线性分类任务，是因为数据发生了空间变换。
<center>
![](http://myblog-1253290602.file.myqcloud.com/longlong-blog/1-1.png)
</center>

- 其中**x**是输入向量，**y**是输出向量，**b**是偏移向量，W是权重矩阵，a()是激活函数。每一层仅仅是把输入**x**经过如此简单的操作得到**y**。

- 数学理解：通过如下5种对输入空间（输入向量的集合）的操作，完成 **输入空间**到**输出空间** 的变换 (矩阵的行空间到列空间)。注：用“空间”二字的原因是被分类的并不是单个事物，而是一类事物。空间是指这类事物所有个体的集合。
1.  升维/降维
2.  放大/缩小
3.  旋转
4.  平移
5.  “弯曲”
	1,2,3的操作由W·**x**完成，4的操作是由+**b**完成，5的操作则是由a()来实现。

### **图解数据变换**
**单层神经网络处理非线性分类任务：**
<center>
<img src="http://myblog-1253290602.file.myqcloud.com/longlong-blog/network2.png" width = "350" height = "350"/>
</center>


**双层神经网络处理非线性分类任务： **
<center>
<img src="http://myblog-1253290602.file.myqcloud.com/longlong-blog/network3.png" width = "350" height = "350"/>
</center>
　　两层神经网络的决策分界是非常平滑的曲线，而且分类的很好。单层网络只能做线性分类任务，为什么两个线性分类任务结合就可以做非线性分类任务？把**输出层的决策分界**单独拿出来看一下。
<center>
<img src="http://myblog-1253290602.file.myqcloud.com/longlong-blog/network4.png" width = "350" height = "350"/>
</center>
　　可以看到，输出层的决策分界仍然是直线。关键就是，从输入层到隐藏层时，数据发生了空间变换。也就是说，两层神经网络中，**隐藏层对原始的数据进行了一个空间变换，使其可以被线性分类，然后输出层的决策分界划出了一个线性分类分界线，对其进行分类。**

**总结：**

- **矩阵和向量相乘，本质上就是对向量的坐标空间进行一个变换。**因此，隐藏层的参数矩阵的作用就是使得数据的原始坐标空间从线性不可分，转换成了线性可分。
- 这里有非常棒的[可视化空间变换demo](http://cs.stanford.edu/people/karpathy/convnetjs//demo/classify2d.html)，打开尝试并感受这种扭曲过程。更多内容请看[Neural Networks, Manifolds, and Topology](http://link.zhihu.com/?target=http%3A//colah.github.io/posts/2014-03-NN-Manifolds-Topology/)
- 从线性可分视角来说：神经网络的学习就是学习如何利用矩阵的线性变换加激活函数的非线性变换，将原始输入空间投向线性可分/稀疏的空间去分类/回归。
- 增加节点数目的：增加维度，即增加线性转换能力。
- 增加层数目的：增加激活函数的次数，即增加非线性转换次数。
- [Tensorflow playground](http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.64338&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)展示了数据是如何“流动”，very nice！！！

